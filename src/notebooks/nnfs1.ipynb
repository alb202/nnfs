{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# from pandas import DataFrame\n",
    "# import string\n",
    "import pickle\n",
    "# https://www.geeksforgeeks.org/implementation-of-neural-network-from-scratch-using-numpy/\n",
    "# https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('targets.pkl', 'wb') as file:\n",
    "#     pickle.dump(targets, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('targets.pkl', 'wb') as file:\n",
    "    targets = pickle.loads(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = dict()\n",
    "# A\n",
    "targets[\"a\"] = [0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]\n",
    "\n",
    "# B\n",
    "targets[\"b\"] = [0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0]\n",
    "\n",
    "# C\n",
    "targets[\"c\"] = [0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0]\n",
    "\n",
    "# D\n",
    "targets[\"d\"] = [0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0]\n",
    "\n",
    "# E\n",
    "targets[\"e\"] = [0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0]\n",
    "\n",
    "# F\n",
    "targets[\"f\"] = [0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "# G\n",
    "targets[\"g\"] = [0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0]\n",
    "\n",
    "# H\n",
    "targets[\"h\"] = [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0]\n",
    "\n",
    "# I\n",
    "targets[\"i\"] = [0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n",
    "\n",
    "# J\n",
    "targets[\"j\"] = [0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n",
    "\n",
    "# # K\n",
    "# targets[\"k\"] = [0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]\n",
    "\n",
    "number_of_targets = len(targets)\n",
    "\n",
    "# Creating labels\n",
    "training_labels = np.zeros((number_of_targets, number_of_targets))\n",
    "for i in range(0, number_of_targets):\n",
    "    training_labels[i, i] = 1\n",
    "# targets[\"y\"] = y\n",
    "\n",
    "\n",
    "# with open(\"targets.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(targets, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting data and labels into numpy array\n",
    "\n",
    "\"\"\"\n",
    "Convert the matrix of 0 and 1 into one hot vector \n",
    "so that we can directly feed it to the neural network,\n",
    "these vectors are then stored in a list x.\n",
    "\"\"\"\n",
    "\n",
    "# Labels are also converted into NumPy array\n",
    "# y: np.array = np.array(targets.get('y'))\n",
    "# y: np.array = targets.pop('y')\n",
    "# training_labels: np.array = training_labels\n",
    "training_data: list = np.array([[np.array(value)] for value in targets.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A\n",
    "# a =[0, 0, 1, 1, 0, 0,\n",
    "#    0, 1, 0, 0, 1, 0,\n",
    "#    1, 1, 1, 1, 1, 1,\n",
    "#    1, 0, 0, 0, 0, 1,\n",
    "#    1, 0, 0, 0, 0, 1]\n",
    "# # B\n",
    "# b =[0, 1, 1, 1, 1, 0,\n",
    "#    0, 1, 0, 0, 1, 0,\n",
    "#    0, 1, 1, 1, 1, 0,\n",
    "#    0, 1, 0, 0, 1, 0,\n",
    "#    0, 1, 1, 1, 1, 0]\n",
    "# # C\n",
    "# c =[0, 1, 1, 1, 1, 0,\n",
    "#    0, 1, 0, 0, 0, 0,\n",
    "#    0, 1, 0, 0, 0, 0,\n",
    "#    0, 1, 0, 0, 0, 0,\n",
    "#    0, 1, 1, 1, 1, 0]\n",
    " \n",
    "# # Creating labels\n",
    "# y =[[1, 0, 0],\n",
    "#    [0, 1, 0],\n",
    "#    [0, 0, 1]]\n",
    "\n",
    "# x =[np.array(a).reshape(1, 30), np.array(b).reshape(1, 30), \n",
    "#                                 np.array(c).reshape(1, 30)]\n",
    " \n",
    "# number_of_targets = len(x)\n",
    "\n",
    "# # Labels are also converted into NumPy array\n",
    "# y = np.array(y)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set input parameters\n",
    "image_x_dimension = 5\n",
    "image_y_dimension = 6\n",
    "image_z_dimension = 1\n",
    "image_input_dimension = image_x_dimension * image_y_dimension * image_z_dimension\n",
    "number_of_output_classes = len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input data validation functions\n",
    "def validate_training_data(data: np.ndarray, n: int, x: int, y: int, z: int = 1) -> bool:\n",
    "    data_shape = data.shape\n",
    "\n",
    "    print(f'Number of samples in training data: {data_shape[0]}')\n",
    "    print(f'Depth of each sample in training data: {data_shape[1]}')\n",
    "    print(f'Length of array for each sample in training data: {data_shape[2]}')\n",
    "\n",
    "    assert data_shape[0] == n, f\"The number of training samples in the data is {data_shape[0]}; expected {n}\"\n",
    "    assert data_shape[1] == z, f\"The depth of each training sample is {data_shape[1]}; expected {z}\"\n",
    "    assert data_shape[2] == (x * y), f\"The length of each 1d training array is {data_shape[2]}; expected {x*y}\"\n",
    "\n",
    "### Input label validation functions\n",
    "def validate_training_labels(labels: np.ndarray, n: int) -> bool:\n",
    "    data_shape = labels.shape\n",
    "\n",
    "    print(f'Number of output classes in target data: {data_shape[0]}')\n",
    "    print(f'Length of array for each target label: {data_shape[1]}')\n",
    "\n",
    "    assert data_shape[0] == data_shape[0], f\"The target labels are shape {data_shape[0]} x {data_shape[1]}; expected a square\"\n",
    "    assert data_shape[0] == n, f\"The number of target labels is {data_shape[0]}; expected {n}\"\n",
    "    assert data_shape[1] == n, f\"The length of each target label array is {data_shape[1]}; expected {n}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training data: 10\n",
      "Depth of each sample in training data: 1\n",
      "Length of array for each sample in training data: 30\n",
      "Number of output classes in target data: 10\n",
      "Length of array for each target label: 10\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the training data\n",
    "validate_training_data(data=training_data, n=number_of_output_classes, x=image_x_dimension, y=image_y_dimension, z=image_z_dimension)\n",
    "validate_training_labels(labels=training_labels, n=number_of_output_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # converting data and labels into numpy array\n",
    "\n",
    "# \"\"\"\n",
    "# Convert the matrix of 0 and 1 into one hot vector \n",
    "# so that we can directly feed it to the neural network,\n",
    "# these vectors are then stored in a list x.\n",
    "# \"\"\"\n",
    "# x: list = [np.array(values).reshape(1, 30) for values in targets.values()]\n",
    "\n",
    "# # Labels are also converted into NumPy array\n",
    "# # y: np.array = np.array(targets.get('y'))\n",
    "# y: np.array = targets.get('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = generate_wt(30, 15)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def sigmoid(x: np.ndarray[np.ndarray[float, ...]]) -> np.ndarray[np.ndarray[float, ...]]:\n",
    "\treturn 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x: np.ndarray[np.ndarray[float, ...]]) -> np.ndarray[np.ndarray[float, ...]]:\n",
    "\treturn np.maximum(0, x)\n",
    "\n",
    "def tanh(x: np.ndarray[np.ndarray[float, ...]]) -> np.ndarray[np.ndarray[float, ...]]:\n",
    "\treturn (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "\n",
    "hidden_activation_function = sigmoid\n",
    "output_activation_function = sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def sigmoid_derivative(x: np.ndarray[np.ndarray[float, ...]]) -> np.ndarray[np.ndarray[float, ...]]:\n",
    "\treturn x * (1.0 - x)\n",
    "\n",
    "# def relu(x: np.ndarray[np.ndarray[float, ...]]) -> np.ndarray[np.ndarray[float, ...]]:\n",
    "# \treturn np.maximum(0, x)\n",
    "\n",
    "# def tanh(x: np.ndarray[np.ndarray[float, ...]]) -> np.ndarray[np.ndarray[float, ...]]:\n",
    "# \treturn (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "\n",
    "# hidden_activation_function = sigmoid\n",
    "# output_activation_function = sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = np.random.randn(500)\n",
    "# DataFrame({'original': tmp, 'transformed': sigmoid(tmp)}).plot.scatter(x='original', y='transformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = generate_wt(2, 3)\n",
    "b = sigmoid(a)\n",
    "c = sigmoid_derivative(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21057962,  0.68410793, -1.8593119 ],\n",
       "       [ 0.58063704, -0.0169199 ,  0.3952963 ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55245123, 0.66465493, 0.13478328],\n",
       "       [0.64121398, 0.49577013, 0.59755702]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24724887, 0.22288875, 0.11661674],\n",
       "       [0.23005861, 0.24998211, 0.24048263]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Feed forward neural network\n",
    "# 1 Input layer(1, 30)\n",
    "# 1 hidden layer (1, 5)\n",
    "# 1 output layer(3, 3)\n",
    "\n",
    "# def f_forward(x, w1, w2) -> np.ndarray[np.ndarray[float]]:\n",
    "# \t# hidden\n",
    "# \tz1: np.ndarray[np.ndarray[float]] = x.dot(w1)# input from layer 1\n",
    "# \ta1: np.ndarray[np.ndarray[float]] = hidden_activation_function(z1)# out put of layer 2 \n",
    "\n",
    "# \t# Output layer\n",
    "# \tz2: np.ndarray[np.ndarray[float]] = a1.dot(w2)# input of out layer\n",
    "# \ta2: np.ndarray[np.ndarray[float]] = output_activation_function(z2)# output of out layer\n",
    "# \treturn(a2)\n",
    "\n",
    "def f_forward(*weights, x) -> np.ndarray[np.ndarray[float, ...]]:\n",
    "\n",
    "\toutput: np.ndarray[np.ndarray[float]] = None\n",
    "\n",
    "\tfor i, w in enumerate(weights):\n",
    "\n",
    "\t\tif i == 0:\n",
    "\t\t\tz: np.ndarray[np.ndarray[float]] = x.dot(w)# input from layer 1\n",
    "\t\telse:\n",
    "\t\t\tz: np.ndarray[np.ndarray[float]] = output.dot(w)# input from layer 1\n",
    "\n",
    "\t\tif i < len(weights) - 1:\n",
    "\t\t\toutput: np.ndarray[np.ndarray[float]] = hidden_activation_function(z)# out put of layer 2 \n",
    "\t\telse:\n",
    "\t\t\toutput: np.ndarray[np.ndarray[float]] = output_activation_function(z)# out put of layer 2 \n",
    "\n",
    "\treturn(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the weights randomly\n",
    "def generate_wt(x: int, y: int) -> np.ndarray:\n",
    "    return np.random.randn(x, y)\n",
    "\t# list_of_weights: list = []\n",
    "\t# for _ in range(x * y):\n",
    "\t# \tlist_of_weights.append(np.random.randn())\n",
    "\t# return(np.array(list_of_weights).reshape(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_wt(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loss we will be using mean square error(MSE)\n",
    "def loss(out: np.ndarray[np.ndarray[float]], Y: np.ndarray) -> float:\n",
    "\t# print('out', out, type(out))\n",
    "\t# print('Y', Y, type(Y))\n",
    "\ts: float = np.square(out-Y)\n",
    "\ts: float = np.sum(s) / len(Y)\n",
    "\treturn(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Back propagation of error \n",
    "# def back_propagation(*weights, x, y, alpha: float) -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "# \tw1 = weights[0]\n",
    "# \tw2 = weights[1]\n",
    "\n",
    "# \t# print('x', x.shape)\n",
    "# \t# print('y', y.shape)\n",
    "\n",
    "# \t# # hidden layer\n",
    "# \t# z1: np.ndarray[np.ndarray[float]] = x.dot(w1) # input from layer 1 \n",
    "# \t# layer_output_1: np.ndarray[np.ndarray[float]] = hidden_activation_function(z1) # output of layer 2 \n",
    "# \t# # print('z1', z1, type(z1), type(z1[0]))\n",
    "# \t# # print('a1', a1, type(a1), type(a1[0]))\n",
    "\n",
    "# \t# # print('z1', z1.shape)\n",
    "# \t# # print('a1', z1.shape)\n",
    "\n",
    "# \t# # Output layer\n",
    "# \t# z2: np.ndarray[np.ndarray[float]] = layer_output_1.dot(w2) # input of out layer\n",
    "# \t# layer_output_2: np.ndarray[np.ndarray[float]] = output_activation_function(z2) # output of out layer\n",
    "\t\n",
    "# \toutputs: np.ndarray[np.ndarray[float]] = []\n",
    "\n",
    "# \tfor i, w in enumerate(weights):\n",
    "\n",
    "# \t\tif i == 0:\n",
    "# \t\t\tz: np.ndarray[np.ndarray[float]] = x.dot(w)# input from layer 1\n",
    "# \t\telse:\n",
    "# \t\t\tz: np.ndarray[np.ndarray[float]] = outputs[i-1].dot(w)# input from layer 1\n",
    "\n",
    "# \t\tif i < len(weights) - 1:\n",
    "# \t\t\toutput: np.ndarray[np.ndarray[float]] = hidden_activation_function(z)# out put of layer 2 \n",
    "# \t\telse:\n",
    "# \t\t\toutput: np.ndarray[np.ndarray[float]] = output_activation_function(z)# out put of layer 2 \n",
    "\t\t\n",
    "# \t\toutputs.append(output)\n",
    "\t\n",
    "# \tlayer_output_1 = outputs[0]\n",
    "# \t# layer_output_2 = outputs[1]\n",
    "\n",
    "# \toutput_error: np.ndarray[np.ndarray[float]] = (outputs[-1] - y)\n",
    "# \toutput_delta = w2.dot(output_error.transpose()).transpose()\n",
    "\t\n",
    "# \tlayer_1_delta = layer_output_1 * (1 - layer_output_1)\n",
    "\t\n",
    "# \t# for each layer, do you multply all the deltas?\n",
    "# \tlayer_1_error: np.ndarray[np.ndarray[float]] = np.multiply(output_delta, layer_1_delta)\n",
    "\n",
    "# \t# print('d2', d2.shape)\n",
    "# \t# print('d1', d1.shape)\n",
    "\n",
    "# \t# Gradient for w1 and w2\n",
    "# \tw1_gradient: np.ndarray[np.ndarray[float]] = x.transpose().dot(layer_1_error)\n",
    "# \tw2_gradient: np.ndarray[np.ndarray[float]] = layer_output_1.transpose().dot(output_error)\n",
    "\n",
    "# \t# Updating parameters\n",
    "# \tupdated_w1: np.ndarray[np.ndarray[float]] = weights[0]-(alpha*(w1_gradient))\n",
    "# \tupdated_w2: np.ndarray[np.ndarray[float]] = weights[1]-(alpha*(w2_gradient))\n",
    "\n",
    "# \treturn(updated_w1, updated_w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back propagation of error \n",
    "def back_propagation(*weights, x, y, alpha: float) -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "\t# w1 = weights[0]\n",
    "\t# w2 = weights[1]\n",
    "\n",
    "\t# print('x', x.shape)\n",
    "\t# print('y', y.shape)\n",
    "\n",
    "\t# # hidden layer\n",
    "\t# z1: np.ndarray[np.ndarray[float]] = x.dot(w1) # input from layer 1 \n",
    "\t# layer_output_1: np.ndarray[np.ndarray[float]] = hidden_activation_function(z1) # output of layer 2 \n",
    "\t# # print('z1', z1, type(z1), type(z1[0]))\n",
    "\t# # print('a1', a1, type(a1), type(a1[0]))\n",
    "\n",
    "\t# # print('z1', z1.shape)\n",
    "\t# # print('a1', z1.shape)\n",
    "\n",
    "\t# # Output layer\n",
    "\t# z2: np.ndarray[np.ndarray[float]] = layer_output_1.dot(w2) # input of out layer\n",
    "\t# layer_output_2: np.ndarray[np.ndarray[float]] = output_activation_function(z2) # output of out layer\n",
    "\n",
    "\tlayer_outputs: list = []\n",
    "\t# output_errors: list = []\n",
    "\toutput_deltas: list = []\n",
    "\t# output_gradients: list = []\n",
    "\toutput_weights: list = []\n",
    "\n",
    "\tfor i, weight in enumerate(weights):\n",
    "\n",
    "\t\tif i == 0:\n",
    "\t\t\tz: np.ndarray[np.ndarray[float]] = x.dot(weight)# input from layer 1\n",
    "\t\telse:\n",
    "\t\t\tz: np.ndarray[np.ndarray[float]] = layer_outputs[i-1].dot(weight)# input from layer 1\n",
    "\n",
    "\t\tif i < len(weights) - 1:\n",
    "\t\t\toutput: np.ndarray[np.ndarray[float]] = hidden_activation_function(z)# out put of layer 2 \n",
    "\t\telse:\n",
    "\t\t\toutput: np.ndarray[np.ndarray[float]] = output_activation_function(z)# out put of layer 2 \n",
    "\n",
    "\t\tlayer_outputs.append(output)\n",
    "\n",
    "\t# layer_outputs = f_forward(*weights, x=x)\n",
    "\n",
    "\t# print(layer_outputs, len(layer_outputs), len(layer_outputs[0]), len(layer_outputs[0]))\n",
    "\n",
    "\t# layer_output_2 = outputs[1]\n",
    "\t# for i in range(len(layer_outputs) - 1, -1, -1):\n",
    "\tfor i in reversed(range(0, len(layer_outputs))):\n",
    "\t\tif i == (len(layer_outputs) - 1):\n",
    "\t\t\tdelta = (layer_outputs[i] - y)\n",
    "\t\t\t# error = (layer_outputs[i] - y)\n",
    "\t\t\t# delta = weights[-1].dot(error.transpose()).transpose()\n",
    "\t\t\t# output_errors.insert(0, error)\n",
    "\t\t\toutput_deltas.insert(0, delta)\n",
    "\t\telse:\n",
    "\t\t\t# layer_output = layer_outputs[i]\n",
    "\t\t\tlayer_delta = sigmoid_derivative(x=layer_outputs[i])\n",
    "\t\t\tcarried_weights = (weights[i+1].dot((output_deltas[0].transpose()))).transpose()\n",
    "\t\t\t# hidden_layer_error = np.multiply(output_deltas[0], layer_delta)\n",
    "\t\t\thidden_layer_error = np.multiply(carried_weights, layer_delta)\n",
    "\t\t\t# output_errors.insert(0, hidden_layer_error)\n",
    "\t\t\toutput_deltas.insert(0, hidden_layer_error)\n",
    "\n",
    "\t# output_error: np.ndarray[np.ndarray[float]] = (layer_outputs[-1] - y)\n",
    "\t# output_delta = w2.dot(output_error.transpose()).transpose()\n",
    "\n",
    "\t# layer_output_1 = layer_outputs[0]\n",
    "\t# layer_1_delta = layer_output_1 * (1 - layer_output_1)\n",
    "\n",
    "\n",
    "\t# # for each layer, do you multply all the deltas?\n",
    "\t# layer_1_error: np.ndarray[np.ndarray[float]] = np.multiply(output_delta, layer_1_delta)\n",
    "\n",
    "\t# print('d2', d2.shape)\n",
    "\t# print('d1', d1.shape)\n",
    "\tfor i, output_error in enumerate(output_deltas):\n",
    "\t\tif i == 0:\n",
    "\t\t\tgradient = x.transpose().dot(output_error)\n",
    "\t\telse:\n",
    "\t\t\tgradient = layer_outputs[i-1].transpose().dot(output_error)# input from layer 1\n",
    "\t\toutput_weight = weights[i] - (alpha * gradient)\n",
    "\t\toutput_weights.append(output_weight)\n",
    "\t\t# output_gradients.append(gradient)\n",
    "\n",
    "\t# # Gradient for w1 and w2\n",
    "\t# w1_gradient: np.ndarray[np.ndarray[float]] = x.transpose().dot(output_errors[0])\n",
    "\t# w2_gradient: np.ndarray[np.ndarray[float]] = layer_outputs[0].transpose().dot(output_errors[1])\n",
    "\n",
    "\t# # Updating parameters\n",
    "\t# updated_w1: np.ndarray[np.ndarray[float]] = weights[0]-(alpha*(output_gradients[0]))\n",
    "\t# updated_w2: np.ndarray[np.ndarray[float]] = weights[1]-(alpha*(output_gradients[1]))\n",
    "\n",
    "\t# return(updated_w1, updated_w2)\n",
    "\n",
    "\treturn tuple(output_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1, 0]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(range(0, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "tmp = [1,2,3]\n",
    "for i in range(len(tmp)-1, -1, -1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple([2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(*[np.array([1,2, 4]),np.array([3,4,5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(*weights, x, Y, alpha: float = 0.01, epoch: int = 10) -> tuple[list, list, np.ndarray, np.ndarray]:\n",
    "\n",
    "\t# w1 = initial_weights[0]\n",
    "\t# w2 = initial_weights[1]\n",
    "\n",
    "\t# print('x', x, len(x))\n",
    "\taccuracy: list = []\n",
    "\tloss_: list = []\n",
    "\n",
    "\tfor j in range(epoch):\n",
    "\t\tepoch_losses: list = []\n",
    "\t\tfor i in range(len(x)):\n",
    "\t\t\t# out: np.ndarray[np.ndarray[float]] = f_forward(x[i], w1, w2)\n",
    "\t\t\tout: np.ndarray[np.ndarray[float]] = f_forward(*weights, x=x[i])\n",
    "\t\t\tloss_value: float = loss(out=out, Y=Y[i])\n",
    "\t\t\tepoch_losses.append(loss_value)\n",
    "\t\t\tweights = back_propagation(*weights, x=x[i], y=Y[i], alpha=alpha)\n",
    "\t\tprint(\"epochs:\", j + 1, \"======== acc:\", (1-(sum(epoch_losses)/len(x)))*100) \n",
    "\t\taccuracy.append((1-(sum(epoch_losses)/len(x)))*100)\n",
    "\t\tloss_.append(sum(epoch_losses) / len(x))\n",
    "\treturn(weights, accuracy, loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(*weights, x) -> int:\n",
    "\n",
    "\tfeed_forward_output: np.ndarray[np.ndarray[float]] = f_forward(*weights, x=x)\n",
    "\t# print(feed_forward_output, sum(sum(feed_forward_output)))\n",
    "\t# maxm: int = 0\n",
    "\t# k: int = 0\n",
    "\t# for i in range(len(feed_forward_output[0])):\n",
    "\t# \tif(maxm < feed_forward_output[0][i]):\n",
    "\t# \t\tmaxm = feed_forward_output[0][i]\n",
    "\t# \t\tk = i\n",
    "\t# k = feed_forward_output[0].index(max(feed_forward_output[0]))\n",
    "\tk = feed_forward_output.argmax()\n",
    "\tplt.imshow(x.reshape(5, 6))\n",
    "\tplt.show()\n",
    "\n",
    "\treturn k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%timeit\n",
    "# # maxm: int = 0\n",
    "# # k: int = 0\n",
    "# feed_forward_output = np.array([[1.33457852e-03, 9.53068532e-05, 2.73656253e-03, \n",
    "# 2.81552447e-03, 3.79061500e-03, 6.59064529e-03,9.19052338e-05, 4.48017812e-04, 9.83174057e-01,2.74596253e-03, 2.91937077e-05]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# maxm: int = 0\n",
    "# # k: int = 0\n",
    "# for i in range(len(feed_forward_output[0])):\n",
    "#     if(maxm < feed_forward_output[0][i]):\n",
    "#         maxm = feed_forward_output[0][i]\n",
    "#         k = i\n",
    "# k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# k = feed_forward_output.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%timeit\n",
    "# k = feed_forward_output[0].index(max(feed_forward_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = [[0.84815273, 0.04205895, 0.0031815, 0.01468254, 0.00505702, 0.10068009, 0.00123184, 0.0093114, 0.03034115, 0.00106627, 0.00247928]]\n",
    "# Y = y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_wt(*(4, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 1 ======== acc: 84.58572266825415\n",
      "epochs: 2 ======== acc: 88.90047328666493\n",
      "epochs: 3 ======== acc: 91.02187351462311\n",
      "epochs: 4 ======== acc: 92.58680379063402\n",
      "epochs: 5 ======== acc: 93.7777582939541\n",
      "epochs: 6 ======== acc: 94.67174078399042\n",
      "epochs: 7 ======== acc: 95.36746889120768\n",
      "epochs: 8 ======== acc: 95.94199612446657\n",
      "epochs: 9 ======== acc: 96.43971952126917\n",
      "epochs: 10 ======== acc: 96.8795307098643\n",
      "epochs: 11 ======== acc: 97.26767774248732\n",
      "epochs: 12 ======== acc: 97.60670033059682\n",
      "epochs: 13 ======== acc: 97.89967749126247\n",
      "epochs: 14 ======== acc: 98.15111797877705\n",
      "epochs: 15 ======== acc: 98.36632166691484\n",
      "epochs: 16 ======== acc: 98.550573506325\n",
      "epochs: 17 ======== acc: 98.70866532906922\n",
      "epochs: 18 ======== acc: 98.8447299378659\n",
      "epochs: 19 ======== acc: 98.96224887189888\n",
      "epochs: 20 ======== acc: 99.0641282073959\n",
      "epochs: 21 ======== acc: 99.15278811728069\n",
      "epochs: 22 ======== acc: 99.23024500605271\n",
      "epochs: 23 ======== acc: 99.29818089785266\n",
      "epochs: 24 ======== acc: 99.3580007039906\n",
      "epochs: 25 ======== acc: 99.41087944861326\n",
      "epochs: 26 ======== acc: 99.45780140003096\n",
      "epochs: 27 ======== acc: 99.49959259944757\n",
      "epochs: 28 ======== acc: 99.53694790352932\n",
      "epochs: 29 ======== acc: 99.57045341754936\n",
      "epochs: 30 ======== acc: 99.60060504820665\n",
      "epochs: 31 ======== acc: 99.62782380328433\n",
      "epochs: 32 ======== acc: 99.65246838206916\n",
      "epochs: 33 ======== acc: 99.67484552495677\n",
      "epochs: 34 ======== acc: 99.69521852047345\n",
      "epochs: 35 ======== acc: 99.71381420373497\n",
      "epochs: 36 ======== acc: 99.73082872323845\n",
      "epochs: 37 ======== acc: 99.74643230342039\n",
      "epochs: 38 ======== acc: 99.76077318853538\n",
      "epochs: 39 ======== acc: 99.77398091856287\n",
      "epochs: 40 ======== acc: 99.7861690592268\n",
      "epochs: 41 ======== acc: 99.79743748491255\n",
      "epochs: 42 ======== acc: 99.8078742944196\n",
      "epochs: 43 ======== acc: 99.8175574243005\n",
      "epochs: 44 ======== acc: 99.82655601232372\n",
      "epochs: 45 ======== acc: 99.83493155378491\n",
      "epochs: 46 ======== acc: 99.84273888550014\n",
      "epochs: 47 ======== acc: 99.85002702596543\n",
      "epochs: 48 ======== acc: 99.8568398950464\n",
      "epochs: 49 ======== acc: 99.8632169324236\n",
      "epochs: 50 ======== acc: 99.86919363066603\n",
      "epochs: 51 ======== acc: 99.87480199608139\n",
      "epochs: 52 ======== acc: 99.88007094826966\n",
      "epochs: 53 ======== acc: 99.88502666749231\n",
      "epochs: 54 ======== acc: 99.8896928974791\n",
      "epochs: 55 ======== acc: 99.8940912100696\n",
      "epochs: 56 ======== acc: 99.89824123707496\n",
      "epochs: 57 ======== acc: 99.90216087390708\n",
      "epochs: 58 ======== acc: 99.90586645882725\n",
      "epochs: 59 ======== acc: 99.90937293108557\n",
      "epochs: 60 ======== acc: 99.91269397073795\n",
      "epochs: 61 ======== acc: 99.91584212252114\n",
      "epochs: 62 ======== acc: 99.91882890582407\n",
      "epochs: 63 ======== acc: 99.92166491250602\n",
      "epochs: 64 ======== acc: 99.92435989406803\n",
      "epochs: 65 ======== acc: 99.92692283947775\n",
      "epochs: 66 ======== acc: 99.92936204477193\n",
      "epochs: 67 ======== acc: 99.93168517541116\n",
      "epochs: 68 ======== acc: 99.93389932223388\n",
      "epochs: 69 ======== acc: 99.93601105174675\n",
      "epochs: 70 ======== acc: 99.93802645139426\n",
      "epochs: 71 ======== acc: 99.93995117037082\n",
      "epochs: 72 ======== acc: 99.94179045646668\n",
      "epochs: 73 ======== acc: 99.94354918938112\n",
      "epochs: 74 ======== acc: 99.94523191088216\n",
      "epochs: 75 ======== acc: 99.94684285214821\n",
      "epochs: 76 ======== acc: 99.9483859585865\n",
      "epochs: 77 ======== acc: 99.94986491239\n",
      "epochs: 78 ======== acc: 99.95128315306353\n",
      "epochs: 79 ======== acc: 99.95264389612443\n",
      "epochs: 80 ======== acc: 99.95395015015967\n",
      "epochs: 81 ======== acc: 99.95520473240138\n",
      "epochs: 82 ======== acc: 99.95641028296522\n",
      "epochs: 83 ======== acc: 99.9575692778802\n",
      "epochs: 84 ======== acc: 99.95868404102497\n",
      "epochs: 85 ======== acc: 99.9597567550736\n",
      "epochs: 86 ======== acc: 99.96078947154274\n",
      "epochs: 87 ======== acc: 99.9617841200232\n",
      "epochs: 88 ======== acc: 99.96274251666983\n",
      "epochs: 89 ======== acc: 99.96366637201682\n",
      "epochs: 90 ======== acc: 99.96455729817825\n",
      "epochs: 91 ======== acc: 99.9654168154882\n",
      "epochs: 92 ======== acc: 99.9662463586294\n",
      "epochs: 93 ======== acc: 99.9670472822945\n",
      "epochs: 94 ======== acc: 99.96782086641983\n",
      "epochs: 95 ======== acc: 99.96856832102799\n",
      "epochs: 96 ======== acc: 99.96929079071217\n",
      "epochs: 97 ======== acc: 99.96998935879145\n",
      "epochs: 98 ======== acc: 99.97066505116473\n",
      "epochs: 99 ======== acc: 99.97131883988739\n",
      "epochs: 100 ======== acc: 99.9719516464932\n"
     ]
    }
   ],
   "source": [
    "# w1: np.ndarray = generate_wt(image_input_dimension, 15)\n",
    "# w2: np.ndarray = generate_wt(15, number_of_targets)\n",
    "# # print(w1, \"\\n\\n\", w2)\n",
    "\n",
    "nn_layers = [\n",
    "    (image_input_dimension, 50), \n",
    "    # (100, 1000), \n",
    "    # (1000, 50), \n",
    "    (50, number_of_targets)\n",
    "    ]\n",
    "initial_weights = [generate_wt(*layer_dimensions) for layer_dimensions in nn_layers]\n",
    "\n",
    "\"\"\"The arguments of train function are data set list x, \n",
    "correct labels y, weights w1, w2, learning rate = 0.1, \n",
    "no of epochs or iteration.The function will return the\n",
    "matrix of accuracy and loss and also the matrix of \n",
    "trained weights w1, w2\"\"\"\n",
    "\n",
    "trained_weights, accuracy_results, loss_results = train(*initial_weights, x=training_data, Y=training_labels, alpha=0.1, epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy\n",
    "plt.plot(accuracy_results)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel(\"Epochs:\")\n",
    "plt.show()\n",
    "\n",
    "# plotting Loss\n",
    "plt.plot(loss_results)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel(\"Epochs:\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The predict function will take the following arguments:\n",
    "1) image matrix\n",
    "2) w1 trained weights\n",
    "3) w2 trained weights\n",
    "\"\"\"\n",
    "\n",
    "# target_characters = list(string.ascii_lowercase)[0: number_of_targets]\n",
    "random_target_character: str = random.choice(list(targets.keys()))\n",
    "random_target_array: np.array = targets.get(random_target_character)\n",
    "random_target_character, random_target_array\n",
    "\n",
    "print('Character to predict: ', random_target_character)\n",
    "predict_array = np.asarray(random_target_array).reshape(1, 30)\n",
    "\n",
    "output = predict(*trained_weights, x=predict_array)\n",
    "('Predicted letter: ', output, list(targets.keys())[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tmp = np.array([[0.55808578 ,0.9860626 , 0.9742969,  0.00516912 ,0.98360591 ,0.06539192,\n",
    "  0.00998272 ,0.02447332, 0.24575064 ,0.99329634, 0.30919278 ,0.7771927,\n",
    "  0.51599149, 0.95019494 ,0.01692725]] )\n",
    "print(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(tmp, (1-tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.multiply(tmp, (1-tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "tmp * (1-tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(sum(output[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(np.asarray(list(targets.values())[1]).reshape(5, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
